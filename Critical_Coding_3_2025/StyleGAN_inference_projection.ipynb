{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "687c9dd0-32ee-4941-a0de-f36bd6fd5e16",
   "metadata": {},
   "source": [
    "##### Reused code from [StyleGAN3 Inversion](https://github.com/ouhenio/stylegan3-projector)\n",
    "\n",
    "# Generating/ projecting images using pre-trained StyleGAN3 models  \n",
    "\n",
    "In this notebook, we're going to look at the [StyleGAN3](https://github.com/NVlabs/stylegan3) (aka Alias-Free GAN) image generation model. We'll walkthrough how to generate random images from the model, and interpolating between them. We'll also cover how to project (encode) an image into a StyleGAN latent space and find a close match.  \n",
    "\n",
    "Compared to DCGAN, StyleGAN is a model architecture that can generate images with much higher resolution, higher quality and diversity. We'll be using the third version of it (StyleGAN3). Different from StyleGAN and StyleGAN2, StyleGAN3 models images as continuous signals (shown in the figure below) so that it is free from the [texture sticking](https://www.youtube.com/watch?v=-2hLdOonvK0) effect, and it can be trained very well on unaligned datasets!\n",
    "\n",
    "<img src='./notebook_ims/stylegan3-teaser-1920x1006.png' width='500px'></img>\n",
    "\n",
    "**Note:**  \n",
    "Due to StyleGAN's requirement for a few customised C++ operations, it's likely that this notebook **cannot run on Windows devices without NVIDIA GPU**. If you miss any library, please refer to StyleGAN3's [requirements](https://github.com/NVlabs/stylegan3?tab=readme-ov-file#requirements). A suggestion is to first try running it on your device, and if anything goes wrong then move to Colab. \n",
    "\n",
    "<a href=\"https://colab.research.google.com/drive/1ak3Yakel1BMqHfrgQi12GOZAQ-6Zf1FQ\" target=\"_blank\"><img src=\"./notebook_ims/colab-badge.svg\" height=22></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0822df3c-561e-4501-8234-34d7bf12f13e",
   "metadata": {},
   "source": [
    "## 00. Clone repo, installation, download models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649cfa3a-4413-4733-99bc-be7ad219a621",
   "metadata": {},
   "source": [
    "#### Clone the official StyleGAN3 repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33308bd-bd77-4b3a-892a-dc3f715d978c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVlabs/stylegan3.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9dff34-64be-4994-a3f2-83a239bb28e2",
   "metadata": {},
   "source": [
    "#### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980d3b8-fdec-4c54-8bcd-cdf8e9d7a53e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ninja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d794e3-410a-4971-8ad7-09a5bfec140c",
   "metadata": {},
   "source": [
    "#### Download pre-trained models\n",
    "\n",
    "We'll use pre-trained StyleGAN models, there are a lot more different models [here](https://github.com/NVlabs/stylegan3?tab=readme-ov-file#additional-material) and [here](https://github.com/justinpinkney/awesome-pretrained-stylegan3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42f4dc-9b52-4c08-8376-0a3f6c3569d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl -L 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/research/stylegan2/1/files?redirect=true&path=stylegan2-ffhq-1024x1024.pkl' -o stylegan2-ffhq-1024x1024.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210d671-45ae-46b7-ab74-672645599d46",
   "metadata": {},
   "source": [
    "if the above `curl` command doesn't work, try `wget` instead (or just navigate to the url and download it and place the file next to this notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6660d19-b4da-4958-8c3a-ec3b52701dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget --content-disposition 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/research/stylegan2/1/files?redirect=true&path=stylegan2-ffhq-1024x1024.pkl' -O stylegan2-ffhq-1024x1024.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57b8c97-f7c9-4faf-8aeb-4c4f5d0a59f5",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72b26b-0edd-416d-965d-20007eba53e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "base_dir = 'stylegan3'\n",
    "sys.path.append(f'{base_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3c043-85d2-4cbc-afcd-9e23b65b7c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch_utils import misc\n",
    "from torch_utils.ops import upfirdn2d\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.nn.functional import interpolate\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import legacy\n",
    "import dnnlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb431a75",
   "metadata": {},
   "source": [
    "Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b801df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slerp(val, low, high):\n",
    "    '''\n",
    "    original: Animating Rotation with Quaternion Curves, Ken Shoemake\n",
    "    Code: https://github.com/soumith/dcgan.torch/issues/14, Tom White\n",
    "    '''\n",
    "    if len(low.shape) == 1:\n",
    "        omega = np.arccos(np.dot(low/np.linalg.norm(low), high/np.linalg.norm(high)))\n",
    "        so = np.sin(omega)\n",
    "        return np.sin((1.0-val)*omega) / so * low + np.sin(val*omega)/so * high\n",
    "    elif len(low.shape) == 2:\n",
    "        ws = []\n",
    "        for i in range(low.shape[0]):\n",
    "            omega = np.arccos(np.dot(low[i,:]/np.linalg.norm(low[i,:]), high[i,:]/np.linalg.norm(high[i,:])))\n",
    "            so = np.sin(omega)\n",
    "            w = np.sin((1.0-val)*omega) / so * low[i,:] + np.sin(val*omega)/so * high[i,:]\n",
    "            ws.append(w)\n",
    "        return torch.tensor(np.array(ws))\n",
    "\n",
    "\n",
    "def get_perceptual_loss(synth_image, target_features, perceptual_model):\n",
    "    # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
    "    synth_image = (synth_image + 1) * (255/2)\n",
    "    if synth_image.shape[2] > 256:\n",
    "        synth_image = interpolate(synth_image, size=(256, 256), mode='area')\n",
    "\n",
    "    # Features for synth images.\n",
    "    synth_features = perceptual_model(synth_image, resize_images=False, return_lpips=True)\n",
    "    return (target_features - synth_features).square().sum()\n",
    "\n",
    "def get_target_features(target, perceptual_model, device):\n",
    "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
    "    if target_images.shape[2] > 256:\n",
    "        target_images = interpolate(target_images, size=(256, 256), mode='area')\n",
    "    return perceptual_model(target_images, resize_images=False, return_lpips=True)\n",
    "\n",
    "\n",
    "def run_projector(projection_target, g_model, steps, perceptual_model, device, save_path = None):\n",
    "  zs = torch.randn([10000, g_model.mapping.z_dim], device=device)\n",
    "  w_stds = g_model.mapping(zs, None).std(0)\n",
    "\n",
    "  target_features = get_target_features(projection_target, perceptual_model, device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    qs = []\n",
    "    losses = []\n",
    "    for _ in range(8):\n",
    "      q = (g_model.mapping(torch.randn([4,g_model.mapping.z_dim], device=device), None, truncation_psi=0.75) - g_model.mapping.w_avg) / w_stds\n",
    "      images = g_model.synthesis(q * w_stds + g_model.mapping.w_avg)\n",
    "      loss = get_perceptual_loss(images, target_features, perceptual_model)\n",
    "      i = torch.argmin(loss)\n",
    "      qs.append(q[i])\n",
    "      losses.append(loss)\n",
    "    qs = torch.stack(qs)\n",
    "    losses = torch.stack(losses)\n",
    "    i = torch.argmin(losses)\n",
    "    q = qs[i].unsqueeze(0).requires_grad_()\n",
    "\n",
    "  # Sampling loop\n",
    "  q_ema = q\n",
    "  opt = torch.optim.AdamW([q], lr=0.20, betas=(0.0,0.999))\n",
    "  target_images = projection_target.unsqueeze(0).to(device).to(torch.float32)/255*2-1\n",
    "\n",
    "  for i in range(steps):\n",
    "    opt.zero_grad()\n",
    "    w = q * w_stds\n",
    "    image = g_model.synthesis(w + g_model.mapping.w_avg, noise_mode='const')\n",
    "    # in the first quarter of the steps, use MSE loss, then switch to perceptual loss\n",
    "    if i > (steps / 4):\n",
    "      loss = get_perceptual_loss(image, target_features, perceptual_model)\n",
    "      if i % 10 == 0:\n",
    "        print(f\"image {i}/{steps} | perceptual_loss: {loss.item()}\")\n",
    "    else:\n",
    "      loss = torch.nn.functional.mse_loss(image, target_images)\n",
    "      if i % 10 == 0:\n",
    "        print(f\"image {i}/{steps} | mse_loss: {loss}\")\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    q_ema = q_ema * 0.9 + q * 0.1\n",
    "    image = g_model.synthesis(q_ema * w_stds + g_model.mapping.w_avg, noise_mode='const')\n",
    "\n",
    "    if save_path is not None:\n",
    "        pil_image = to_pil_image(image[0].add(1).div(2).clamp(0,1))\n",
    "        pil_image.save(f'{save_path}/{i:04}.jpg')\n",
    "\n",
    "  return q_ema * w_stds + g_model.mapping.w_avg\n",
    "\n",
    "def image_path_to_tensor(target_image_filename, model_resolution):\n",
    "    target_pil = Image.open(target_image_filename).convert('RGB')\n",
    "    w, h = target_pil.size\n",
    "    s = min(w, h)\n",
    "    target_pil = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n",
    "    target_pil = target_pil.resize((model_resolution, model_resolution), Image.LANCZOS)\n",
    "    target_uint8 = np.array(target_pil, dtype=np.uint8)\n",
    "    target_tensor = torch.tensor(target_uint8.transpose([2, 0, 1]))\n",
    "    return target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158b357-6e72-4d84-b08e-5d9944d6a7e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f'torch version {torch.__version__}')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9488af4-e150-4048-a688-43338889910b",
   "metadata": {},
   "source": [
    "## 01. Load a StyleGAN model\n",
    "\n",
    "Make sure the `network_pkl` match the path to the `.pkl` file we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82748239-e57f-45c3-b0ea-4e474284b39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "network_pkl = 'stylegan2-ffhq-1024x1024.pkl'\n",
    "\n",
    "with dnnlib.util.open_url(network_pkl) as f:\n",
    "    model = legacy.load_network_pkl(f)\n",
    "    g_model = model['G'].eval().requires_grad_(False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9011dbe-204d-4f68-b855-e5f5e90e9e19",
   "metadata": {},
   "source": [
    "## 02. Interpolation\n",
    "\n",
    "We'll start with randomly sample two latent vectors and interpolate between them. We'll generate is a short video in the z latent space of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de2dfd-61fb-44ee-9579-5bb0f3d46e63",
   "metadata": {},
   "source": [
    "#### Create random latent vector\n",
    "\n",
    "Firsr, create two random latent vectors with seed, and convert them to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b33f2b-a9f9-4758-8ba4-cd77df10242f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "z1 = np.random.randn(1, 512)\n",
    "z1 = torch.Tensor(z1).to(device)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "z2 = np.random.randn(1, 512)\n",
    "z2 = torch.Tensor(z2).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749042a0-000e-4457-8dac-1c43b2bab66d",
   "metadata": {},
   "source": [
    "#### Forward pass and generation\n",
    "\n",
    "Forward pass the two latent vector tensors to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d142b7f-557f-4a48-aaf9-0198622d1cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img1 = g_model(z1, None, truncation_psi=0.6)[0]\n",
    "img2 = g_model(z2, None, truncation_psi=0.6)[0]\n",
    "\n",
    "display(to_pil_image(torch.cat((img1,img2),dim=2).add(1).div(2).clamp(0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d355a-ee71-4b76-b977-61e8243cc902",
   "metadata": {},
   "source": [
    "#### Create interpolated latent vectors  \n",
    "\n",
    "In order to interpolate between two latent vectors, we're going to use a slerp function to sample between them, this will create a spherical interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33529f18-9972-4eb2-8286-cb56503927b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import slerp\n",
    "from base64 import b64encode\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4155bd8-d7b4-4c8f-95d7-2ad079454c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define how many vectors we want\n",
    "num_interp = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34085be-f13d-4e77-aa2a-d7f4e3505c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create intervals\n",
    "interp_vals = np.linspace(1./num_interp, 1, num=num_interp)\n",
    "\n",
    "# Convert latent vectors to numpy arrays\n",
    "latent_a_np = z1.cpu().numpy().squeeze()\n",
    "latent_b_np = z2.cpu().numpy().squeeze()\n",
    "\n",
    "# Create our spherical interpolation between two points\n",
    "latent_interp = np.array([slerp(v, latent_a_np, latent_b_np) for v in interp_vals], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2109d510-8813-48bd-bd44-ff82ef675ce5",
   "metadata": {},
   "source": [
    "Now we have 100 latent vectors, each vector has 512 dimensions (512 is the dimensionality of the latent vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ced66-46ab-4a63-9cc6-fff8d1f60a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_interp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930e0ae-c042-4d9c-beb4-a3308ccfc528",
   "metadata": {},
   "source": [
    "#### Generate animated frames using all latent vectors  \n",
    "\n",
    "First, create a folder to store all generated frames, then we're going to loop through all latent vectors and forward pass each of them to generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11dde9-e80f-4b60-885c-d93537c8f01f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_name = 'animation_frames_stylegan'\n",
    "\n",
    "# create a directory to save the images\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72bef4a-7a19-47b8-bf3c-c50495bbebd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Array for images to save to for visualisation\n",
    "img_list = []\n",
    "\n",
    "# For each latent vector in our interpolation\n",
    "for i,latent in enumerate(latent_interp):\n",
    "    # Convert to torch tensor\n",
    "    latent = torch.tensor(latent).unsqueeze(0).to(device)\n",
    "    # Generate image from latent\n",
    "    image_tensor = g_model(latent, None, truncation_psi=0.6)\n",
    "    # Convert to PIL Image\n",
    "    image = transforms.functional.to_pil_image(image_tensor.clamp(-1,1).add(1).div(2).cpu().squeeze(0))\n",
    "    image.save(f'./{folder_name}/{i:05}.jpg')\n",
    "    # Add to image array\n",
    "    img_list.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606eb07d-c8bd-4c27-bb41-ca9cc3616cba",
   "metadata": {},
   "source": [
    "#### Create a video using generated images as video frames  \n",
    "\n",
    "We will need [FFMPEG](https://ffmpeg.org/) to do create a video from generated frames:\n",
    "\n",
    "##### Installing FFMPEG\n",
    "\n",
    "If you want to make the gif animation using this code notebook you will need to install [ffmpeg](https://ffmpeg.org/download.html). We have already installed ffmpeg in week 4, so you should already have it. But if you don't, then you can follow these instructions:\n",
    "\n",
    "To install FFMPEG on Mac:\n",
    "- Step 1: [install homebrew](https://brew.sh/)\n",
    "- Step 2: Run `brew install ffmpeg`\n",
    "\n",
    "To install FFMPEG on Windows:\n",
    "- Follow [these instructions](https://phoenixnap.com/kb/ffmpeg-windows) for Windows installation\n",
    "\n",
    "To install FFMPEG on Ubuntu linux:\n",
    "- Step 1: Run `sudo apt update`\n",
    "- Step 2: Run `sudo apt install ffmpeg`\n",
    "\n",
    "If you cannot install FFMPEG you can [make a gif manually using this website](https://ezgif.com/maker), or use other video editing softwares that can process frames into videos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d31b620-3370-49c3-89d5-334aa2d1ad99",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ffmpeg -framerate 30 -i ./{folder_name}/%05d.jpg animation_frames_stylegan.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db810819-88f9-416a-a1ac-514b3d16c4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp4 = open('animation_frames_stylegan.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=512 controls loop>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60237ea-2740-4a44-9c9d-933324b1f50a",
   "metadata": {},
   "source": [
    "## 03. Projection into latent space\n",
    "\n",
    "We have shown what can be done, with some sampling of random latent variables in the z space, now we are going upload an image of anyone you choose, and project it into StyleGAN's latent space. \n",
    "\n",
    "A StyleGAN model has a fully connected mapping network that transform a latent vector to a series of \"style latent vector\", called the $w$ space. Each synthesis layer in the StyleGAN generator applies a style latent vector. Here we're projecting the input image into the $w$ space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5258136-0cbd-4588-bed6-8e7a578ec8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stylegan3.dnnlib.util import open_url\n",
    "from utils import image_path_to_tensor\n",
    "from utils import run_projector\n",
    "\n",
    "from utils import slerp\n",
    "from base64 import b64encode\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a230dc-99dd-481a-bc15-9b0683d985fb",
   "metadata": {},
   "source": [
    "#### Fetch a feature extractor\n",
    "\n",
    "In order to move closer to the target style vector, we'll be using a pre-trained feature extractor to tell us how close we are. We'll use [VGG16](https://www.geeksforgeeks.org/vgg-16-cnn-model/) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e4b0e-173b-4f09-b40c-eb6244396f35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
    "with open_url(url) as f:\n",
    "    vgg16 = torch.jit.load(f).eval().to(device)\n",
    "print('Using device:', device, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f596d1e2-3312-419b-8246-a6bcdc2668b2",
   "metadata": {},
   "source": [
    "#### Prepare an image  \n",
    "\n",
    "You'll need to align the image of the face into similar settings (position, rotation) used for the dataset.\n",
    "\n",
    "If you're using another image, make sure to change the path here to your image file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0128f91-c6a8-4cc5-bd88-e01117a0e6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_image1_filename = \"../media/b.jpg\"\n",
    "target_image2_filename = \"../media/k.jpg\"\n",
    "\n",
    "target_tensor1 = image_path_to_tensor(target_image1_filename, g_model.img_resolution).to(device)\n",
    "target_tensor2 = image_path_to_tensor(target_image2_filename, g_model.img_resolution).to(device)\n",
    "print(target_tensor1.shape)\n",
    "print(target_tensor2.shape)\n",
    "print('inputs:')\n",
    "display(to_pil_image(torch.cat((target_tensor1,target_tensor2),dim=2).div(255).clamp(0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850072f3-29dd-46ea-9959-250a02b25ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a directory to save the model checkpoints\n",
    "if not os.path.exists('animation_frames_stylegan_projector'):\n",
    "    os.makedirs('animation_frames_stylegan_projector')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72396d22-bf8f-4e83-a7e6-824799a23f45",
   "metadata": {},
   "source": [
    "#### Project into the $w$ space\n",
    "\n",
    "We are now going to take our image, and project it into the $w$ space of StyleGAN. This process will start with a random vector, and make changes to the latent vector and noise input, until it converges on on the closest matching image in StyleGAN space to our input image. This is quite a long process, however if you want to shorten it you can change the the `step` variable to a smaller number if you want to reduce the amount of steps taken to find the closest match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2999a-354a-41b8-a083-43ae3aea7bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps = 500\n",
    "\n",
    "ws_ema1 = run_projector(projection_target = target_tensor1,\n",
    "                       g_model = g_model, \n",
    "                       steps = steps,\n",
    "                       perceptual_model = vgg16, \n",
    "                       device = device, \n",
    "                       save_path = None)\n",
    "\n",
    "ws_ema2 = run_projector(projection_target = target_tensor2,\n",
    "                       g_model = g_model, \n",
    "                       steps = steps,\n",
    "                       perceptual_model = vgg16, \n",
    "                       device = device, \n",
    "                       save_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07590a7-9950-4202-adab-1021014adcc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image1 = g_model.synthesis(ws_ema1, noise_mode='const')[0]\n",
    "image2 = g_model.synthesis(ws_ema2, noise_mode='const')[0]\n",
    "\n",
    "display(to_pil_image(torch.cat((image1,image2),dim=2).add(1).div(2).clamp(0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7a88f-e94f-4c10-aefe-d486928e1241",
   "metadata": {},
   "source": [
    "#### Interpolating the projections\n",
    "\n",
    "Now we have our closest match in projected image space, depending on how closely the chosen image conforms to the content and style of photos in the Flickr Faces High Quality (FFHQ) dataset usually determines how closely we are going to be able to find a match.\n",
    "\n",
    "We can also interpolate between these two projections by interpolating in the $w$ space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ec07358-1780-4a47-bec0-9f59351a47b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_interp = 100\n",
    "\n",
    "# create intervals\n",
    "interp_vals = np.linspace(1./num_interp, 1, num=num_interp)\n",
    "\n",
    "# Convert latent vectors to numpy arrays\n",
    "latent_a_np = ws_ema1.cpu().detach().numpy().squeeze()\n",
    "latent_b_np = ws_ema2.cpu().detach().numpy().squeeze()\n",
    "\n",
    "# Create our spherical interpolation between two points\n",
    "latent_interp = np.array([slerp(v, latent_a_np, latent_b_np) for v in interp_vals], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b46a86e-8c74-489f-9846-527fd27734b0",
   "metadata": {},
   "source": [
    "Now we have 100 latent style vectors $w$, each vector has (num_of_layers, 512) dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1f8ba-6863-41e8-b409-98a2f15e92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_interp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8ac22-f7ae-4b79-a5f2-4b4a93ac1938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_name = 'animation_frames_stylegan_projector'\n",
    "\n",
    "# create a directory to save the images\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0af7a-940e-4c31-9c91-60f0d5712a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Array for images to save to for visualisation\n",
    "img_list = []\n",
    "\n",
    "# For each latent vector in our interpolation\n",
    "for i,latent in enumerate(latent_interp):\n",
    "    # Convert to torch tensor\n",
    "    latent = torch.tensor(latent).unsqueeze(0).to(device)\n",
    "    # Generate image from latent\n",
    "    image_tensor = g_model.synthesis(latent, noise_mode='const')\n",
    "    # Convert to PIL Image\n",
    "    image = transforms.functional.to_pil_image(image_tensor.clamp(-1,1).add(1).div(2).cpu().squeeze(0))\n",
    "    image.save(f'./{folder_name}/{i:05}.jpg')\n",
    "    # Add to image array\n",
    "    img_list.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a8baf-25cf-4d69-9093-3d79dcf72f32",
   "metadata": {},
   "source": [
    "#### Create an intrpolation video  \n",
    "\n",
    "We'll use FFMPEG again to create an intrpolation video  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a63c8-97fe-4fdd-89a4-1c1cc8fe9d91",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ffmpeg -framerate 30 -i ./{folder_name}/%05d.jpg animation_frames_stylegan_projector.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f09e5-c631-439c-b578-29450107a3b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp4 = open('animation_frames_stylegan_projector.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls loop>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb87b6-6fa3-4693-8e98-695a618f700a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
