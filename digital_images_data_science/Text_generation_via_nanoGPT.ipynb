{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is adapted from Terence Broad's [AI for Media](https://git.arts.ac.uk/tbroad/AI-4-Media-23-24/) unit, with some adaptations by Adam Cole, Rebecca Fiebrink, and Jasper Zheng."
      ],
      "metadata": {
        "id": "30QliRPatWKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a nanoGPT model and generate texts"
      ],
      "metadata": {
        "id": "emAEbrnxo_3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to select a **T4 GPU** runtime for this notebook! (in the \"Runtime\" menu -> \"Change runtime type\" -> select \"T4 GPU\" and click save."
      ],
      "metadata": {
        "id": "4d2QbWXEwsNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading data and installation"
      ],
      "metadata": {
        "id": "bMW0uyWGvQqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/jasper-zheng/teaching/blob/main/digital_images_data_science/nanoGPT.zip?raw=true -O nanoGPT.zip\n",
        "!wget https://github.com/jasper-zheng/teaching/blob/main/digital_images_data_science/nanoGPT_data.zip?raw=true -O nanoGPT_data.zip\n"
      ],
      "metadata": {
        "id": "WbyhafWvurmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nanoGPT.zip\n",
        "!unzip nanoGPT_data.zip"
      ],
      "metadata": {
        "id": "rJ6FPtXDph_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "JKWYjKkqpmGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni6sTnE3o-z_"
      },
      "source": [
        "# Train a GPT model from scratch\n",
        "\n",
        "Here we are going to train a GPT model from scratch (babyGPT) using characters as tokens.\n",
        "\n",
        "This is based on [Andrej Karpathy's nanoGPT](https://github.com/karpathy/nanoGPT), a really simple and lightweight implementation of a large language model. This code was chosen as the most accessible codebase where you can inspect and understand the code behind transformers/LLMs, as well as easily train and customise your own simple GPT models.\n",
        "\n",
        "The code here has been modified a bit to make it simpler, more Pythonic, and more easy for you to adapt for your own project. But the core of the technical implementation in `nanoGPT/model.py` is still quite complicated, it's okay if it's a bit confuse now because we haven't had any machine learning/ neural network class.\n",
        "\n",
        "For now, the focus is on how we **tokenise** the Shakespeare document into a dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from nanoGPT.data import prepare_dataset_as_chars, prepare_dataset_gpt_tokenizer"
      ],
      "metadata": {
        "id": "JcbhwmDgpcl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_path = 'data/shakespeare.txt'\n",
        "dataset_path = 'text-datasets'"
      ],
      "metadata": {
        "id": "42EYyOlnpqgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_dataset_gpt_tokenizer(text_path, os.path.join(dataset_path, 'shakespeare_tokens'))"
      ],
      "metadata": {
        "id": "Xc8MXaCUp1hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ2n-rHPo-0B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import yaml\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "from yaml.loader import SafeLoader\n",
        "from contextlib import nullcontext\n",
        "\n",
        "from nanoGPT.model import GPT\n",
        "from nanoGPT.config import ModelConfig, TrainingConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoLSQJwto-0C"
      },
      "source": [
        "### Setup parameters\n",
        "If you are on an M1 macbook or have an NVIDIA GPU, you may want to change the device from cpu to `mps` or `cuda` for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcBE1Dewo-0C"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# I/O\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
        "\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
        "\n",
        "# train a miniature character-level shakespeare model\n",
        "# good for debugging and playing on macbooks and such\n",
        "# if fine-tuning a gpt model you want to make these bigger\n",
        "eval_interval = 50 # keep frequent because we'll overfit\n",
        "eval_iters = 50\n",
        "log_interval = 10 # don't print too too often\n",
        "\n",
        "# we expect to overfit on this small dataset, so only save when val improves\n",
        "always_save_checkpoint = False\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "ctx = nullcontext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGLtF0aoo-0D"
      },
      "source": [
        "### Set training config and dataset\n",
        "\n",
        "In the folder `configs` there are a number of yaml files with different configs for the model and training of our GPT models. You can look in there if you want to see how these are set up. By default this notebook will use `baby_gpt_config.yaml` which will use characters as tokens so we can train a simple and small GPT model for educational purposes. For good quality results though, you will want to fine-tune a pretrained GPT2 model using the second config set here.\n",
        "\n",
        "This code has been designed for you to train these on whatever custom text dataset you want, using the code in `00-prepare-dataset.ipynb`. Just use the *chars* or *tokens* dataset depending on whether you want to train a babyGPT or GPT-2 model.\n",
        "\n",
        "##### OPTION 1: Train a babyGPT character model from scratch:\n",
        "\n",
        "This configuration will train a babyGPT model from scratch at the character level (similar to [CharRNN](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)). This is designed to be small enough to be trained from scratch on a laptop cpu.\n",
        "\n",
        "Start with this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUUCJmBIo-0D"
      },
      "outputs": [],
      "source": [
        "config_path = 'data/baby_gpt_config.yaml'\n",
        "dataset_dir = 'text-datasets/shakespeare_tokens'\n",
        "ckpt_dir = 'ckpt/shakespeare_tokens'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLj0I5NZo-0E"
      },
      "source": [
        "### Load model and training hyperparameters\n",
        "\n",
        "Here we will load our Model and Training hyperparameters into our ModelConfig and TrainingConfig classes, which we will call `m` and `t`. This is different to the original nanoGPT code and was built in order to make something that was more robust and Pythonic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQvESx1go-0E"
      },
      "outputs": [],
      "source": [
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    m_dict, t_dict = list(yaml.load_all(f, Loader=SafeLoader))\n",
        "\n",
        "m = ModelConfig.from_dict(m_dict['model_config'])\n",
        "t = TrainingConfig.from_dict(t_dict['training_config'])\n",
        "print(m.__dict__)\n",
        "print(t.__dict__)\n",
        "tokens_per_iter = gradient_accumulation_steps* t.batch_size * m.block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnjczhyxo-0E"
      },
      "source": [
        "### Data loader\n",
        "\n",
        "Here we load in our data from our pre-calculated binaries `.bin` of our list of token indicies for our text sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c_kB8B4o-0E"
      },
      "outputs": [],
      "source": [
        "train_data = np.memmap(os.path.join(dataset_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(dataset_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - m.block_size, (t.batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+m.block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+m.block_size]).astype(np.int64)) for i in ix])\n",
        "    if device == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_9hIbIRo-0E"
      },
      "source": [
        "### Initialise model\n",
        "\n",
        "Here we initialise our model. It gets a bit complicated as we want to override the params in our ModelConfig `m` if there is a conflict with the params listed in a saved checkpoint that we load in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jiy-y2doo-0E"
      },
      "outputs": [],
      "source": [
        "# init these up here, can override if t.init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(dataset_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "if t.init_from == 'scratch':\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    m.vocab_size = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    model = GPT(m)\n",
        "elif t.init_from == 'resume':\n",
        "    print(f\"Resuming training from {ckpt_dir}\")\n",
        "    # resume training from a checkpoint.\n",
        "    ckpt_path = os.path.join(ckpt_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    # force these config attributes to be equal otherwise we can't even resume training\n",
        "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "    m = ModelConfig.from_dict(checkpoint_model_args)\n",
        "    # create the model\n",
        "    model = GPT(m)\n",
        "    state_dict = checkpoint['model']\n",
        "    # fix the keys of the state dictionary :(\n",
        "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint['iter_num']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "elif t.init_from.startswith('gpt2'):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {t.init_from}\")\n",
        "    # initialize from OpenAI GPT-2 weights\n",
        "    override_args = dict(dropout=m.dropout)\n",
        "    model = GPT.from_pretrained(t.init_from, override_args)\n",
        "    # read off the created config params, so we can store them into checkpoint correctly\n",
        "    m = ModelConfig.from_dict(model.config.__dict__)\n",
        "# crop down the model block size if desired, using model surgery\n",
        "if m.block_size < model.config.block_size:\n",
        "    model.crop_block_size(m.block_size)\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUTxPeueo-0F"
      },
      "source": [
        "### Setup core objects\n",
        "\n",
        "Here we setup our core objects like our optimiser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOBbvltKo-0F"
      },
      "outputs": [],
      "source": [
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(t.weight_decay, t.learning_rate, (t.beta1, t.beta2), device)\n",
        "if t.init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None # free up memory\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBH9T3Pqo-0F"
      },
      "source": [
        "### Util functions\n",
        "\n",
        "Some utility functions to help us estimate the loss and get the learning rate (if we set it to gradually step down over training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLO9zamKo-0F"
      },
      "outputs": [],
      "source": [
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it, t):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < t.warmup_iters:\n",
        "        return t.learning_rate * it / t.warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > t.lr_decay_iters:\n",
        "        return t.min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - t.warmup_iters) / (t.lr_decay_iters - t.warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return t.min_lr + coeff * (t.learning_rate - t.min_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuXka4bWo-0F"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Here is our training loop. The code is a bit more complex here than we have seen before. This is largely because there is quite a bit of boilerplate code which can be used to make training more efficient on NVIDIA GPUs.\n",
        "\n",
        "While this code is running, start doing Task 2 in the text block below this code. (i.e., look at `nanoGPT/model.py` and look for the specified code functionalities.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X53FEJ1Co-0F"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num, t) if t.decay_lr else t.learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': m.__dict__,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': model.config.__dict__,\n",
        "                    'dataset': dataset_dir\n",
        "                }\n",
        "                print(f\"saving checkpoint to {ckpt_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(ckpt_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if t.grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), t.grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = model.estimate_mfu(t.batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > t.max_iters:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO25WKMOo-0F"
      },
      "source": [
        "## Generate texts with trained nanoGPT\n",
        "\n",
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "import tiktoken\n",
        "\n",
        "from contextlib import nullcontext\n",
        "\n",
        "from nanoGPT.model import GPT\n",
        "from nanoGPT.config import ModelConfig"
      ],
      "metadata": {
        "id": "gsQ96CIkrZxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "ckpt_dir = 'ckpt/shakespeare_tokens' # ignored if init_from is not 'resume'\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu' (standard), 'cuda' (NVIDIA GPU), 'mps' (Mac M1/M2/M3)\n",
        "dtype = 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "ctx = nullcontext()"
      ],
      "metadata": {
        "id": "7O1bLJmSrbsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start with a prompt"
      ],
      "metadata": {
        "id": "AVgM0IF8rdmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = \"Once upon a time\""
      ],
      "metadata": {
        "id": "gfif2Qwtrhbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and generate"
      ],
      "metadata": {
        "id": "tIVVR6_Rrlz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "if init_from == 'resume':\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(ckpt_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    m = ModelConfig.from_dict(checkpoint_model_args)\n",
        "    model = GPT(m)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "elif init_from.startswith('gpt2'):\n",
        "    # init from a given GPT-2 model\n",
        "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
        "\n",
        "# look for the meta pickle in case it is available in the dataset folder\n",
        "load_meta = False\n",
        "if init_from == 'resume' and 'dataset' in checkpoint: # older checkpoints might not have these...\n",
        "    meta_path = os.path.join(checkpoint['dataset'], 'meta.pkl')\n",
        "    load_meta = os.path.exists(meta_path)\n",
        "if load_meta:\n",
        "    print(f\"Loading meta from {meta_path}...\")\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
        "    stoi, itos = meta['stoi'], meta['itos']\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])\n",
        "else:\n",
        "    # ok let's assume gpt-2 encodings by default\n",
        "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "    decode = lambda l: enc.decode(l)\n",
        "\n",
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "id": "lhEXFMSGriZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QjQNH0KwsL4l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}