{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasper-zheng/teaching/blob/main/digital_images_data_science/Text_classifier_via_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a text classifier using word embeddings  \n",
        "\n",
        "The dataset used in this example is [fine-food reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews) from Amazon. The dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of this dataset, consisting of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text).  \n",
        "\n",
        "We'll use the [Word2vec](https://radimrehurek.com/gensim/models/word2vec.html) model from Gensim for word embeddings."
      ],
      "metadata": {
        "id": "4qHlG7vIba8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/jasper-zheng/teaching/blob/main/digital_images_data_science/reviews_10k.csv?raw=true -O reviews_10k.csv\n"
      ],
      "metadata": {
        "id": "6pGtzGhGxWAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "BAnPeczFYgEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader"
      ],
      "metadata": {
        "id": "bDVGptalbTeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the word embedding"
      ],
      "metadata": {
        "id": "2pCqnYraYujx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_w2v = gensim.downloader.load('glove-twitter-100')"
      ],
      "metadata": {
        "id": "P67xFiAtg8r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_text(text):\n",
        "    vectors = [model_w2v[word] for word in text if word in model_w2v]\n",
        "    if vectors:\n",
        "      return torch.tensor(sum(vectors) / len(vectors))\n",
        "    else:\n",
        "      return torch.zeros(100) # Handle cases with no recognized words\n",
        "\n",
        "\n",
        "def get_cosine_similarity(vec_a, vec_b):\n",
        "        dot_product = vec_a @ vec_b\n",
        "        product_of_magnitudes = np.linalg.norm(vec_a) * np.linalg.norm(vec_b)\n",
        "        return dot_product / product_of_magnitudes"
      ],
      "metadata": {
        "id": "RrF8RDaMY7_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting our word embedding model:"
      ],
      "metadata": {
        "id": "Bfh9hdeOd8Ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dog = vectorize_text(\"dog\")\n",
        "cat = vectorize_text(\"cat\")\n",
        "computers = vectorize_text(\"computers\")"
      ],
      "metadata": {
        "id": "EPOvJmNmcigd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'distance between dog and cat is {get_cosine_similarity(dog, cat)}')\n",
        "print(f'distance between dog and computers is {get_cosine_similarity(dog, computers)}')\n",
        "print(f'distance between cat and computers is {get_cosine_similarity(cat, computers)}')"
      ],
      "metadata": {
        "id": "--uEdtJNcido"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A larger distance means that our embedding thinks the two words are far away from each other, so this is accurate!"
      ],
      "metadata": {
        "id": "L6UHeOc3gBnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_w2v.most_similar('computer', topn=10)"
      ],
      "metadata": {
        "id": "Hc_Ta7oogy7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Issues and Biases in word embeddings:  \n",
        "\n",
        "Word embeddings for quantitative analysis can be quite problematic, especially in terms of gender, racial, class, sexuality, disability or other... Here we expose some examples:"
      ],
      "metadata": {
        "id": "ukHESwCBeYX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doctor = vectorize_text(\"doctor\")\n",
        "woman = vectorize_text(\"woman\")\n",
        "man = vectorize_text(\"man\")\n",
        "\n",
        "print(f'distance between doctor and woman is {get_cosine_similarity(doctor, woman)}')\n",
        "print(f'distance between doctor and man is {get_cosine_similarity(doctor, man)}')\n",
        "print('please be critical when using word embeddings')"
      ],
      "metadata": {
        "id": "3yvSrOVJdBC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try out some other words to see if you can reveal some other problematic terms in the embedding."
      ],
      "metadata": {
        "id": "7blsBCZdBv_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the embedding to create a food review dataset"
      ],
      "metadata": {
        "id": "rVbn1S0IeBdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df = pd.read_csv('reviews_10k.csv')\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "reviews_df = reviews_df.dropna()  # Remove rows with missing values\n",
        "reviews = reviews_df['Text'].apply(lambda x: x.lower().split()).tolist()  # Tokenize text\n",
        "reviews_df.head(3)"
      ],
      "metadata": {
        "id": "pHwC0rsth6FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_vectors = [vectorize_text(review) for review in reviews]\n",
        "scores = reviews_df['Score'].values\n"
      ],
      "metadata": {
        "id": "imrRBYmRY_O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode scores to numerical values (if needed)\n",
        "le = LabelEncoder()\n",
        "scores_encoded = le.fit_transform(scores)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews_vectors, scores_encoded, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "8XSNforYZAhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create PyTorch dataset\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Classifier Model\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "train_dataset = ReviewDataset(X_train, y_train)\n",
        "test_dataset = ReviewDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n"
      ],
      "metadata": {
        "id": "rOkXK4TsZC8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Neural Network Classifier Model"
      ],
      "metadata": {
        "id": "wlhb8COYoEsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextClassifier(100, len(le.classes_))  # Assuming 100-dim word embeddings\n",
        "\n",
        "# Training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "_SWaY-K2Ynhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Classifier Model"
      ],
      "metadata": {
        "id": "FsAdpQ5MoKYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30  # Adjust as needed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 2 == 0:\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "ja8Bz3N4ZHD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting some results"
      ],
      "metadata": {
        "id": "UiMF_HHpoN9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_score(text, model, le, device):\n",
        "    text_vector = vectorize_text(text.lower().split())\n",
        "    text_vector = text_vector.to(device)\n",
        "    with torch.no_grad():\n",
        "      output = model(text_vector)\n",
        "      _, predicted = torch.max(output, 0)\n",
        "    return le.inverse_transform([predicted.item()])[0]"
      ],
      "metadata": {
        "id": "6zaBc5F4ahRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"This is an amazing product! I highly recommend it.\"\n",
        "predicted_score = predict_score(new_text, model, le, device)\n",
        "print(f\"Predicted score: {predicted_score}\")"
      ],
      "metadata": {
        "id": "XNF49D5YZj4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"The cheesecake is not as advertised\"\n",
        "predicted_score = predict_score(new_text, model, le, device)\n",
        "print(f\"Predicted score: {predicted_score}\")"
      ],
      "metadata": {
        "id": "JuS29ev8aWlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gr_-9atdahz6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}